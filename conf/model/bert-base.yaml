init:
  doc_model: 'google-bert/bert-base-uncased'
  tokenizer: ${model.init.doc_model}
  device: 'cuda'
  aggregation_mode: 'mean'
  embedding_size: 768
  save_model: 'bert-base'
  specialized_mode: 'sbmoe_all'
  max_tokenizer_length: 256
  normalize: False
  temperature: 1
adapters:
  num_experts_to_use: 6
  num_experts: 6
  residual: True
  latent_size: 192
  non_linearity: 'relu'
  use_adapters: True

continue_train: False
  